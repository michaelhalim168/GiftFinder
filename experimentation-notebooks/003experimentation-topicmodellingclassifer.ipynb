{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gift Recommender Engine: Topic Modelling Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will attempt to improve the performance of my model. As of now, my working approach is this: train a classifier using Reddit data extracted from different categories. This model is then fed with a user's Twitter data that is filtered by performing sentiment analysis (extracting only positive tweets). When tested on Reddit dataset, the models perform extremely well (up to 98% accuracy). I evaluated the model on a celebrity's user profile to predict what that celebrity might like - in this case, Taylor Swift and overall, recommended some pretty relevant gift categories for her: music, movies, and books. However, the approach is quite messy - I have to input each Tweet individually into the classifier and count the most frequent topics from each Tweet. One significant limitation of this approach is that not every tweet, even though it has a positive sentiment, can be used to recommend a gift. It would make more sense to identify the relevant keywords that make up a topic and use these keywords as inputs to a classifier. However, performing LDA on every user's Tweets is inpractical for this process. Thus, I wonder if it's possible to use the output of an LDA model as input for a supervised learning classification problem.\n",
    "\n",
    "In this notebook, I will attempt to create a classifier that predicts gift categories from vectors that correspond to the distribution of topics identified by the LDA model. I will use the Reddit dataset I scraped earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Clean Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/reddit-categories-clean2.csv')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "data = df[['category', 'all-text', 'clean-text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "punctuations = string.punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_lemmatize(text):\n",
    "    if type(text) == list:\n",
    "        doc = nlp(u\"{}\".format(' '.join(text)))\n",
    "    else:\n",
    "        doc = nlp(u\"{}\".format(text))\n",
    "    lemmatized = list()\n",
    "    for token in doc:\n",
    "        lemmatized.append(token.lemma_)\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.split() #split into list\n",
    "    text = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.MULTILINE) for s in text] #remove any links\n",
    "    text = [s.lower() for s in text] #convert every character into lowercase\n",
    "    text = [re.sub(rf\"[{string.punctuation}]\", \" \", s) for s in text] #remove punctuations\n",
    "    text = [re.sub(r'[0-9]', ' ', s) for s in text] #remove all digits\n",
    "    text = ' '.join(text)  #resplits\n",
    "    text = [s for s in text.split() if len(s) >= 2] #removes words with one word length\n",
    "    text = [s for s in text if s not in stopwords] #remove all stopwords\n",
    "    text = ' '.join(spacy_lemmatize(text)) #lemmatize text using spacy and join into a string\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['clean-text'] = data['all-text'].map(preprocess)\n",
    "#data.to_csv('datasets/reddit-categories-clean3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean-text-list'] = data['clean-text'].apply(lambda x: x.split())\n",
    "all_text = data['clean-text-list'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use HDA to Identify Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(all_text)\n",
    "id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "id2word.compactify()\n",
    "corpus = [id2word.doc2bow(text) for text in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "hdp = HdpModel(corpus, id2word, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hdp.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, num_topics=20,\n",
    "                                             id2word=id2word, chunksize=100,\n",
    "                                             workers=6, passes=50,\n",
    "                                             per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(all_text)):\n",
    "    top_topics = lda.get_document_topics(corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(20)]\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = np.array(train_vecs)\n",
    "y = np.array(data.category)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "ref = dict(zip(data['category'].to_numpy(), y))\n",
    "ref = {k:v for k,v in sorted(ref.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24117805998378816"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = OneVsRestClassifier(LinearSVC(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(random_state=0))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2512023777357471"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl-bootcamp",
   "language": "python",
   "name": "lhl-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
