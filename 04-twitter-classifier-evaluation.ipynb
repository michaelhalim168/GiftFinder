{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gift Recommender Engine: Topic Classification Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines the approaches my attempts to evaluate the performance of my topic model on actual tweets. To get a sense of how my model is performing, I will test my classifier on the following tweets:\n",
    "\n",
    "1. <b> Company Tweets </b>: This can be used as a proof-of-concept baseline test of the approach. Companies should be very distinct (ex. ESPN should predict sports, etc), so it may give a picture if the classifier is going on the right approach.\n",
    "2. <b> Celebrity Tweets </b>: Celebrity's Tweets - we have some understanding of what a celebrity may or may not like (whether they are a singer, actor, or athlete). This way, we can test how the model is doing on specific users.\n",
    "3. <b> Random Users Tweets </b>: Dataset on Kaggle containing Tweets for sentiment analysis. I filtered the tweets to that of users who have Tweeted more than 50 times. This is extremely messy data - we'll see how our model/approach filters out noise.\n",
    "\n",
    "The implementation of the model on tweets can be outlined as followed:\n",
    "\n",
    "1. Clean tweets using preprocessing function to remove links, stopwords, etc. \n",
    "2. Perform sentiment analysis on each tweet and filter tweets that contain positive sentiment. The idea here is that if a user tweets positively about something, that means they like that particular topic and can be gifted that item.\n",
    "3. Input each tweet into the topic classifier, built out of a linear support vector classifier. Each tweet will generate a certain topic and a probability (i.e how confident the model is at predicting that particular topic).\n",
    "4. Filter topics that have a confidence threshold greater than a certain point as to filter noise.\n",
    "5. Count the most topics tweeted by e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl-bootcamp",
   "language": "python",
   "name": "lhl-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
